Use the Local LLM of Ollama model(llama3.2)

#Configure on property file
spring.ai.ollama.chat.options.model=llama3.2

#Then pass the parameter by get request as message
